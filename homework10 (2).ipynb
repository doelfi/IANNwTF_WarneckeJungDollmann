{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "050c36bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "#import tensorflow_text as tf_text\n",
    "#import tensorflow_dataset as tf_ds\n",
    "import datetime\n",
    "import pprint\n",
    "import tqdm\n",
    "import re\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "936344c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the text\n",
    "text = open(\"bible.txt\", \"r\").read().lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5df8c8a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# counts how often a word appears in the text \n",
    "def Count_word_frequency(text): \n",
    "    text = text.numpy().tolist()\n",
    "    text.sort()\n",
    "    counts = { text[0] : 1 }\n",
    "    current_word = text[0]\n",
    "    for i in text[1:]: \n",
    "        if i == current_word:\n",
    "            counts[current_word] += 1\n",
    "        else:\n",
    "            current_word = i\n",
    "            counts.update({current_word:1})\n",
    "    counts = {key: val for key, val in sorted(counts.items(), key = lambda ele: ele[1], reverse = True)}\n",
    "    return counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cb81b988",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creates a list of the most common words in a text\n",
    "def Vocabulary(text, size):\n",
    "    vocabulary = tf.convert_to_tensor(list(text.keys())[:size])\n",
    "    return vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a775f7b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def words_to_number(text, voc):\n",
    "    text = text.numpy().tolist()\n",
    "    voc = voc.numpy().tolist()\n",
    "    for item in range(len(text)):\n",
    "        if text[item] in voc:\n",
    "            text[item] = voc.index(text[item])\n",
    "        else: \n",
    "            # UNK = index 10000\n",
    "            text[item] = 10000\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fc6363e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# returns a tensor of tokens of the num_words (int) of most common words\n",
    "def Tokenization(text, num_words):\n",
    "    \n",
    "    #convert everything to lower case\n",
    "    text.lower()\n",
    "\n",
    "    # replace every new-line characters with a white space\n",
    "    text = re.sub('[\\n]', ' ', text)\n",
    "    # and remove every special character\n",
    "    text = re.sub('[^a-zA-Z_ ]', '', text)\n",
    "    \n",
    "    text = tf.strings.split(text)\n",
    "        \n",
    "    # get word frequency to figure out the n most common words\n",
    "    word_frequency = Count_word_frequency(text)\n",
    "    \n",
    "    # create a vocabulary with all common words\n",
    "    vocabulary = Vocabulary(word_frequency, num_words)\n",
    "    \n",
    "    # convert every word into a token\n",
    "    #tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=num_words, oov_token='oov')\n",
    "    #tokens = tokenizer.fit_on_texts([text])\n",
    "    \n",
    "    # replace words not in vocabulary with UNK\n",
    "    # this takes verylong, may need alternatives\n",
    "    length_vocab = len(vocabulary)\n",
    "    for item in list(word_frequency.keys())[length_vocab + 1:]:\n",
    "        text = tf.strings.regex_replace(text, f'^{item}$', 'UNK', replace_global = True)\n",
    "        \n",
    "    # transform the str into int so we can create one-hot encodings\n",
    "    text = words_to_number(text, vocabulary)\n",
    "        \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0af2bd6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# it should create all (input,target) pairs inside the context_window and put them into a dataset\n",
    "# not sure if it does that currently, might have to check that again\n",
    "def Target_pairs(text, num_words):\n",
    "\n",
    "    dataset = tf.data.Dataset.from_tensor_slices(text)\n",
    "\n",
    "    iterator = iter(dataset) \n",
    "    iterator.get_next()\n",
    "    shift1 = dataset.map(lambda x: iterator.get_next())\n",
    "\n",
    "    iterator2 = iter(dataset) \n",
    "    iterator2.get_next()\n",
    "    iterator2.get_next()\n",
    "    shift2 = dataset.map(lambda x: iterator2.get_next())\n",
    "\n",
    "\n",
    "    #Reihenfolge ist egal???? \n",
    "    shift1up = tf.data.Dataset.zip((dataset, shift1))\n",
    "    shift2up = tf.data.Dataset.zip((dataset, shift2))\n",
    "    shift1down = tf.data.Dataset.zip((shift1, dataset))\n",
    "    shift2down = tf.data.Dataset.zip((shift2, dataset))\n",
    "\n",
    "    dataset = shift2down.concatenate(shift1up).concatenate(shift2up).concatenate(shift1down)\n",
    "\n",
    "    # also do we need to turn it into a one-hot encoding?\n",
    "    dataset = dataset.map(lambda inp, target: (tf.one_hot(inp, depth=num_words, dtype=tf.int32),tf.one_hot(target, depth=num_words, dtype=tf.int32)))\n",
    "    \n",
    "    dataset = dataset.shuffle(10000).batch(32).prefetch(tf.data.AUTOTUNE)\n",
    "    \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9229cb48",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-03 15:20:33.438907: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-02-03 15:20:33.442222: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-02-03 15:20:33.442332: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-02-03 15:20:33.442905: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-02-03 15:20:33.443438: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-02-03 15:20:33.443542: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-02-03 15:20:33.443635: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-02-03 15:20:33.701145: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-02-03 15:20:33.701277: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-02-03 15:20:33.701379: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-02-03 15:20:33.701474: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 1236 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3070, pci bus id: 0000:07:00.0, compute capability: 8.6\n"
     ]
    }
   ],
   "source": [
    "# create dataset\n",
    "num_words = 10000\n",
    "dataset = Target_pairs(Tokenization(text,num_words),num_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4471d99d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the actual model\n",
    "class SkipGram(tf.keras.layers.Layer):\n",
    "    \n",
    "    def __init__(self, vocabulary_size, embedding_size):\n",
    "        super(SkipGram,self).__init__()\n",
    "        \n",
    "        self.vocabulary_size = vocabulary_size\n",
    "        self.embedding_size = embedding_size\n",
    "        self.loss_metric = tf.keras.metrics.Mean(name = \"loss\")\n",
    "        \n",
    "    def build(self,string):\n",
    "        # also don't know if this is in any way correct\n",
    "        self.embedding = self.add_weight(shape = (self.vocabulary_size,self.embedding_size), trainable=True)\n",
    "        self.score_matrix = self.add_weight(shape = (self.vocabulary_size, self.embedding_size), trainable=True)\n",
    "        \n",
    "    def call(self, input_word):\n",
    "        # I don't know if this is how you actually use that function\n",
    "        target_predicted = tf.nn.embedding_lookup(params = self.embedding, ids = input_word)\n",
    "        return target_predicted\n",
    "    \n",
    "    def train(self, data):\n",
    "        \n",
    "        input_word, target_word = data\n",
    "        predictions = self(input_word)\n",
    "        # using this gives you dimensional problems\n",
    "        loss_function = tf.keras.losses.CategoricalCrossentropy()\n",
    "        loss = loss_function(target_word,predictions)\n",
    "        # using this gives you other problems\n",
    "        '''loss = tf.reduce_mean(\n",
    "            tf.nn.nce_loss(weights = self.score_matrix,\n",
    "                           biases = None,\n",
    "                          labels = target_word,\n",
    "                          inputs = predictions,\n",
    "                          num_sampled = 2,\n",
    "                          num_classes = self.vocabulary_size))\n",
    "                          '''\n",
    "        \n",
    "        # appearantly this is an outdated thing from tf1, so we probably can't use that\n",
    "        tf.train.AdamOptimizer().minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8f30c501",
   "metadata": {},
   "outputs": [],
   "source": [
    "# svery shorted version\n",
    "def training_loop(model, train_ds, epochs):\n",
    "    \n",
    "    config_name= \"config_name\"\n",
    "    current_time = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "\n",
    "    train_log_path = f\"logs/{config_name}/{current_time}/train/\"\n",
    "\n",
    "    # log writer for training metrics\n",
    "    train_summary_writer = tf.summary.create_file_writer(train_log_path) \n",
    "    \n",
    "    # 1. iterate over epochs\n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        # 2. train step over all batches in training data\n",
    "        for data in tqdm.tqdm(train_ds):\n",
    "            metrics = model.train(data)\n",
    "        \n",
    "            # 3. log and print training metrics \n",
    "        with train_summary_writer.as_default():\n",
    "            tf.summary.scalar(name = \"loss\", data = metrics[\"loss\"], step = epoch)\n",
    "\n",
    "        print([f\"{key}: {value.numpy()}\" for (key, value) in metrics.items()])\n",
    "\n",
    "        # 4. rest metrics\n",
    "        model.reset_metrics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fd0bfc9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                 | 0/98753 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Shapes (32, 10000) and (32, 10000, 64) are incompatible",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [28], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m model \u001b[38;5;241m=\u001b[39m SkipGram(\u001b[38;5;241m10000\u001b[39m,\u001b[38;5;241m64\u001b[39m)\n\u001b[1;32m      3\u001b[0m train_ds \u001b[38;5;241m=\u001b[39m dataset\n\u001b[0;32m----> 4\u001b[0m \u001b[43mtraining_loop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_ds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn [10], line 17\u001b[0m, in \u001b[0;36mtraining_loop\u001b[0;34m(model, train_ds, epochs)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[1;32m     14\u001b[0m \n\u001b[1;32m     15\u001b[0m     \u001b[38;5;66;03m# 2. train step over all batches in training data\u001b[39;00m\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m data \u001b[38;5;129;01min\u001b[39;00m tqdm\u001b[38;5;241m.\u001b[39mtqdm(train_ds):\n\u001b[0;32m---> 17\u001b[0m         metrics \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m         \u001b[38;5;66;03m# 3. log and print training metrics \u001b[39;00m\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m train_summary_writer\u001b[38;5;241m.\u001b[39mas_default():\n",
      "Cell \u001b[0;32mIn [27], line 26\u001b[0m, in \u001b[0;36mSkipGram.train\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m     24\u001b[0m predictions \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m(input_word)\n\u001b[1;32m     25\u001b[0m loss_function \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mlosses\u001b[38;5;241m.\u001b[39mCategoricalCrossentropy()\n\u001b[0;32m---> 26\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mloss_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtarget_word\u001b[49m\u001b[43m,\u001b[49m\u001b[43mpredictions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;124;03m'''loss = tf.reduce_mean(\u001b[39;00m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;124;03m    tf.nn.nce_loss(weights = self.score_matrix,\u001b[39;00m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;124;03m                   biases = None,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;124;03m                  num_classes = self.vocabulary_size))\u001b[39;00m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;124;03m                  '''\u001b[39;00m\n\u001b[1;32m     36\u001b[0m tf\u001b[38;5;241m.\u001b[39mtrain\u001b[38;5;241m.\u001b[39mAdamOptimizer()\u001b[38;5;241m.\u001b[39mminimize(loss)\n",
      "File \u001b[0;32m~/anaconda3/envs/iannwtf-gpu/lib/python3.9/site-packages/keras/losses.py:141\u001b[0m, in \u001b[0;36mLoss.__call__\u001b[0;34m(self, y_true, y_pred, sample_weight)\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    140\u001b[0m   call_fn \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39m__internal__\u001b[38;5;241m.\u001b[39mautograph\u001b[38;5;241m.\u001b[39mtf_convert(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcall, tf\u001b[38;5;241m.\u001b[39m__internal__\u001b[38;5;241m.\u001b[39mautograph\u001b[38;5;241m.\u001b[39mcontrol_status_ctx())\n\u001b[0;32m--> 141\u001b[0m losses \u001b[38;5;241m=\u001b[39m \u001b[43mcall_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    142\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m losses_utils\u001b[38;5;241m.\u001b[39mcompute_weighted_loss(\n\u001b[1;32m    143\u001b[0m     losses, sample_weight, reduction\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_reduction())\n",
      "File \u001b[0;32m~/anaconda3/envs/iannwtf-gpu/lib/python3.9/site-packages/keras/losses.py:245\u001b[0m, in \u001b[0;36mLossFunctionWrapper.call\u001b[0;34m(self, y_true, y_pred)\u001b[0m\n\u001b[1;32m    242\u001b[0m   y_pred, y_true \u001b[38;5;241m=\u001b[39m losses_utils\u001b[38;5;241m.\u001b[39msqueeze_or_expand_dimensions(y_pred, y_true)\n\u001b[1;32m    244\u001b[0m ag_fn \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39m__internal__\u001b[38;5;241m.\u001b[39mautograph\u001b[38;5;241m.\u001b[39mtf_convert(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfn, tf\u001b[38;5;241m.\u001b[39m__internal__\u001b[38;5;241m.\u001b[39mautograph\u001b[38;5;241m.\u001b[39mcontrol_status_ctx())\n\u001b[0;32m--> 245\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mag_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fn_kwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/iannwtf-gpu/lib/python3.9/site-packages/tensorflow/python/util/traceback_utils.py:153\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m--> 153\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    155\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/anaconda3/envs/iannwtf-gpu/lib/python3.9/site-packages/keras/losses.py:1789\u001b[0m, in \u001b[0;36mcategorical_crossentropy\u001b[0;34m(y_true, y_pred, from_logits, label_smoothing, axis)\u001b[0m\n\u001b[1;32m   1784\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m y_true \u001b[38;5;241m*\u001b[39m (\u001b[38;5;241m1.0\u001b[39m \u001b[38;5;241m-\u001b[39m label_smoothing) \u001b[38;5;241m+\u001b[39m (label_smoothing \u001b[38;5;241m/\u001b[39m num_classes)\n\u001b[1;32m   1786\u001b[0m y_true \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39m__internal__\u001b[38;5;241m.\u001b[39msmart_cond\u001b[38;5;241m.\u001b[39msmart_cond(label_smoothing, _smooth_labels,\n\u001b[1;32m   1787\u001b[0m                                \u001b[38;5;28;01mlambda\u001b[39;00m: y_true)\n\u001b[0;32m-> 1789\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mbackend\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcategorical_crossentropy\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1790\u001b[0m \u001b[43m    \u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrom_logits\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfrom_logits\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/iannwtf-gpu/lib/python3.9/site-packages/keras/backend.py:5083\u001b[0m, in \u001b[0;36mcategorical_crossentropy\u001b[0;34m(target, output, from_logits, axis)\u001b[0m\n\u001b[1;32m   5081\u001b[0m target \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mconvert_to_tensor(target)\n\u001b[1;32m   5082\u001b[0m output \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mconvert_to_tensor(output)\n\u001b[0;32m-> 5083\u001b[0m \u001b[43mtarget\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43massert_is_compatible_with\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   5085\u001b[0m \u001b[38;5;66;03m# Use logits whenever they are available. `softmax` and `sigmoid`\u001b[39;00m\n\u001b[1;32m   5086\u001b[0m \u001b[38;5;66;03m# activations cache logits on the `output` Tensor.\u001b[39;00m\n\u001b[1;32m   5087\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(output, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_keras_logits\u001b[39m\u001b[38;5;124m'\u001b[39m):\n",
      "\u001b[0;31mValueError\u001b[0m: Shapes (32, 10000) and (32, 10000, 64) are incompatible"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "model = SkipGram(10000,64)\n",
    "train_ds = dataset\n",
    "training_loop(model, train_ds, epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b2c3cf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir logs/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:iannwtf-gpu]",
   "language": "python",
   "name": "conda-env-iannwtf-gpu-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
