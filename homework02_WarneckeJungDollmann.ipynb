{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Randomly generate 100 numbers between 0 and 1 and save them to an array ’x’. These are your input values.\n",
    "x = np.random.rand(100,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Create an array ’t’. For each entry x[i] in x, calculate x[i]**3-x[i]**2 \n",
    "#    and save the results to t[i]. These are your targets.\n",
    "t = pow(x,3) - pow(x,2) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class for one layer in a MLP\n",
    "class Layer:\n",
    "    def __init__(self, n_units, input_units):\n",
    "        # The constructor should accept an integer argument n_units, indicating the number of units in the layer.\n",
    "        self.n_units = n_units\n",
    "        \n",
    "        # The constructor should accept an integer argument ’input_units’,indicating the number of units in the preceding layer.\n",
    "        self.input_units = input_units\n",
    "        \n",
    "        # The constructor should instantiate a bias vector and a weight matrixof shape (n inputs, n units).\n",
    "        # Use random values for the weights andzeros for the biases. \n",
    "        self.bias_vector = np.zeros(n_units)\n",
    "        self.weights = np.random.rand(input_units,n_units)\n",
    "        \n",
    "        # instantiate empty attributes for layer-input, layer preactivation and layer activation\n",
    "        self.layer_input = 0\n",
    "        self.layer_preactivation = 0\n",
    "        self.layer_activation = 0\n",
    "        \n",
    "    # returns each unit’s activation(i.e. output) using ReLu as the activation function.\n",
    "    def forward_step(self, layer_input):\n",
    "\n",
    "        self.layer_input = layer_input\n",
    "            \n",
    "        self.layer_preactivation = np.matmul(self.layer_input, self.weights) + self.bias_vector\n",
    "        self.layer_activation = np.maximum(0, self.layer_preactivation)\n",
    "        return self.layer_activation\n",
    "    \n",
    "    def backward_step(self,next_layer_derivative):\n",
    "        # heaviside is a step function -> derivative of ReLU\n",
    "        preactivation_derivative = np.heaviside(self.layer_preactivation,0)\n",
    "        \n",
    "        # matmul is a matrix multiplication (not element wise)\n",
    "        weight_gradient = np.matmul(np.transpose(self.layer_input), np.multiply(preactivation_derivative, next_layer_derivative))\n",
    "        \n",
    "        # gradients of the bias vector\n",
    "        bias_gradient = np.multiply(preactivation_derivative, next_layer_derivative)\n",
    "        \n",
    "        # gradients of the inputs\n",
    "        input_gradient = np.matmul(np.multiply(preactivation_derivative, next_layer_derivative),np.transpose(self.weights))\n",
    "        \n",
    "        # update parameters\n",
    "        # rate 0.01 worked so much better then 0.04 -> it actually goes towards zero now\n",
    "        rate = 0.01\n",
    "        self.bias_vector = self.bias_vector - rate * bias_gradient\n",
    "        self.weights = self.weights - rate * weight_gradient\n",
    "        \n",
    "        # return so the next layer can use it for it's calculations\n",
    "        return input_gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLP class which combines instances of Layer class into a MLP\n",
    "# takes a list of Layers: every entry is a layer wit the number of units as the value\n",
    "class MLP:\n",
    "    def __init__(self,structure):\n",
    "        self.layers = list()\n",
    "        for i in range(len(structure)):\n",
    "            # first layer takes one input unit\n",
    "            if (i == 0):\n",
    "                self.layers.append(Layer(structure[i],1))\n",
    "            # for the rest, the number of input units depends on the previous layer\n",
    "            else:\n",
    "                self.layers.append(Layer(structure[i],structure[i-1]))\n",
    "    \n",
    "    # pass input through network\n",
    "    def forward_step(self, x):\n",
    "        activation = x\n",
    "        for layer in self.layers:\n",
    "            activation = layer.forward_step(activation)\n",
    "        return activation\n",
    "    \n",
    "    # just pass through the backwards_step of all layers starting at the last\n",
    "    def backpropagation(self, loss):\n",
    "        next_layer_derivative = loss\n",
    "        for layer in reversed(self.layers):\n",
    "            next_layer_derivative = layer.backward_step(next_layer_derivative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mean squared error function\n",
    "def MSE(y,t):\n",
    "    return 1/2 * (y - t) ** 2\n",
    "\n",
    "# derivative of MSE to feed into the backpropagation\n",
    "def MSE_derivative(y,t):\n",
    "    return y-t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "structure = [10,1]\n",
    "\n",
    "# create MLP\n",
    "mlp = MLP(structure)\n",
    "\n",
    "# train MLP\n",
    "epochs = 1000\n",
    "loss = list()\n",
    "\n",
    "# training\n",
    "\n",
    "for i in range(epochs):\n",
    "    # getting the output for every input\n",
    "    output = mlp.forward_step(x)\n",
    "    current_loss = MSE(output,t)\n",
    "    loss.append(current_loss)\n",
    "    # backpropagation\n",
    "    mlp.backpropagation(MSE_derivative(output,t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'average loss')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEGCAYAAABo25JHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAX9UlEQVR4nO3de7SddX3n8ffn5ELCJQRIEEjAxCGjK21RaUQRdaZeWqB2oktbsLW2KlK6amuna1pxdGZNL3NhnHYpSypDlVodC2O9DXWo4FiXLisiQRFFCISbhIsE5BIM5PqdP/Zz0n3O2Qk7IU92znner7XO2vt59nP2+f5OYH/O7/k9z++XqkKS1F1joy5AkjRaBoEkdZxBIEkdZxBIUscZBJLUcbNHXcCeWrRoUS1btmzUZUjStHL99dc/VFWLB7027YJg2bJlrFmzZtRlSNK0kuTuXb3mqSFJ6jiDQJI6ziCQpI4zCCSp4wwCSeo4g0CSOs4gkKSO60wQrH1gI39x9VoeemLzqEuRpANKZ4Lgtgc3cuE/ruPHP9ky6lIk6YDSmSAIAcB1eCRpou4EQS8HKEwCSerXnSBoHu0RSNJE3QmCPP0xktRFnQmCcfYIJGmiDgVBM1jsGIEkTdCZINg5WGwOSNIE3QmCURcgSQeo7gRBvI9AkgbpThA0j44RSNJE3QkCxwgkaaDuBcFoy5CkA053gmDnXENGgST160wQeNmQJA3WnSBo2B+QpIk6EwROOidJg7UaBElOT7I2ybok5+/muBcl2Z7kjS3W0jwzCSSpX2tBkGQWcBFwBrASeFOSlbs47gLgqrZqAXsEkrQrbfYITgHWVdUdVbUFuBxYPeC43wU+AzzYYi1ePipJu9BmECwB7unbXt/s2ynJEuD1wMW7e6Mk5yZZk2TNhg0b9qoYl6qUpMHaDIJBF2xO/hj+APDuqtq+uzeqqkuqalVVrVq8ePHeFbPzzmKTQJL6zW7xvdcDx/dtLwXum3TMKuDyZiB3EXBmkm1V9fl9XYxDxZI0WJtBcB2wIsly4F7gbOBX+w+oquXjz5N8DPhCGyHQ+wHjP7OVd5ekaau1IKiqbUneSe9qoFnApVV1U5Lzmtd3Oy6wr8VbiyVpoDZ7BFTVlcCVk/YNDICq+s02a9n5czw5JEkTdOfOYgcJJGmg7gRB82gOSNJE3QkCl6qUpIE6FAS9R8cIJGmi7gRB82iPQJIm6k4QONeQJA3UmSDApSolaaDOBEG8n0ySBupMEIyzPyBJE3UmCHZ2CEwCSZqgO0Ewfh+BSSBJE3QnCJpHx4olaaLuBIHTUEvSQN0JgvHLR0dchyQdaLoTBC5VKUkDdSYIxhkDkjRRZ4LAMQJJGqw7QeBSlZI0UGeC4J/ZJZCkfp0JAk8NSdJg3QuC0ZYhSQec7gQBLlUpSYN0JwhcqlKSBupOEDSP9ggkaaLuBIFjBJI0UGeCwKUqJWmwzgSBS1VK0mDdCYJRFyBJB6jOBME4zwxJ0kSdCQKXqpSkwboTBM2jPQJJmqg7QeBcQ5I0UHeCwKUqJWmg7gSBS1VK0kCdCYJxxoAkTdRqECQ5PcnaJOuSnD/g9dVJbkxyQ5I1SV7WXi3NE5NAkiaY3dYbJ5kFXAS8BlgPXJfkiqr6Qd9hXwauqKpKchLwKeB5LdUDePmoJE3WZo/gFGBdVd1RVVuAy4HV/QdU1RP1zyftD6HFv9e9s1iSBmszCJYA9/Rtr2/2TZDk9UluAf4v8LZBb5Tk3ObU0ZoNGzY8o6IcK5akidoMgkF/hE/5GK6qz1XV84DXAX866I2q6pKqWlVVqxYvXrx3xTgNtSQN1GYQrAeO79teCty3q4Or6mvAv0iyqI1iXKpSkgZrMwiuA1YkWZ5kLnA2cEX/AUlOTDOKm+RkYC7wcBvFuFSlJA3W2lVDVbUtyTuBq4BZwKVVdVOS85rXLwbeALwlyVbgSeCsaumOL+cakqTBWgsCgKq6Erhy0r6L+55fAFzQZg07OUYgSQM97amhJO9KsiA9H03y7SQ/vz+K25eCs85J0iDDjBG8raoeB34eWAy8FfhvrVbVAq8akqTBhgmC8dPrZwJ/XVXfZRren+UYgSQNNkwQXJ/kanpBcFWSw4Ad7Za178XV6yVpoGEGi98OvAC4o6o2JTmS3umhaclpqCVpomF6BKcCa6vq0SRvBt4HPNZuWfuek49K0mDDBMGHgU1Jng/8EXA38PFWq2qBS1VK0mDDBMG25iav1cAHq+qDwGHtlrXvuVSlJA02zBjBxiTvAX4deHmzzsCcdstqgUtVStJAw/QIzgI207uf4AF6U0m/v9WqWuBFQ5I02NMGQfPh/0ng8CSvBZ6qquk3RtA82iGQpImGmWLiV4BvAb8M/ApwbZI3tl3YvuZSlZI02DBjBO8FXlRVDwIkWQz8P+DTbRa2r9kjkKTBhhkjGBsPgcbDQ36fJGkaGKZH8MUkVwGXNdtnMWlq6enASeckabCnDYKq+sMkbwBOo3eG5ZKq+lzrle1jLlUpSYMNtTBNVX0G+EzLtbTKpSolabBdBkGSjQw+kxKgqmpBa1W1yB6BJE20yyCoqmk3jcTujHlHmSQN1Jmrf8ZzYMcOuwSS1K8zQTAWJ52TpEE6EwTjJ4Z2OEggSRMMFQRJnp3k1c3z+c1yldPKzlND5oAkTTDMXEPvoDedxP9sdi0FPt9mUW1I0gsDewSSNMEwPYLfoXcz2eMAVXUbcHSbRbUl2COQpMmGCYLNVbVlfCPJbKbpmOtY4g1lkjTJMEHw1ST/Hpif5DXA3wF/325Z7UjsEUjSZMMEwfnABuB7wG/Rm3DufW0W1ZYkDhFI0iTDTDq3A/ir5mtaa+bGGHUZknRAedogSPI9po4JPAasAf6sqh5uo7A29MYIJEn9hpl99B+A7cDfNttnN4+PAx8Dfmnfl9WOxCkmJGmyYYLgtKo6rW/7e0n+qapOS/Lmtgprgz0CSZpqmMHiQ5O8eHwjySnAoc3mtlaqaknvPgKjQJL6DdMjOAe4NMmh9D5LHwfOSXII8F/bLG5fS7yxWJImG+aqoeuAn0lyOJCqerTv5U+1VlkLxsbiVUOSNMlQS1Um+UXgp4B5GZ/OuepPWqyrFU4xIUlTDTPp3MXAWcDv0vss/WXg2cO8eZLTk6xNsi7J+QNe/7UkNzZf30jy/D2sf484xYQkTTXMYPFLq+otwCNV9cfAqcDxT/dNSWYBFwFnACuBNyVZOemwO4F/VVUnAX8KXLInxe8pp5iQpKmGCYKnmsdNSY4DtgLLh/i+U4B1VXVHM2nd5cDq/gOq6htV9Uiz+U16U1y3xikmJGmqYYLg75MsBN4PfBu4C7hsiO9bAtzTt72+2bcrb6d389oUSc5NsibJmg0bNgzxowdziglJmmq3g8VJxoAvN1cKfSbJF4B5VfXYEO+dAfsGfgon+Tl6QfCyQa9X1SU0p41WrVq115/kY/YIJGmK3fYImgnn/rxve/OQIQC9HkD/WMJS4L7JByU5CfgIsLrteYt6YwQmgST1G+bU0NVJ3pDx60aHdx2wIsnyJHPpzVF0Rf8BSU4APgv8elXduofvv8ecYkKSphrmPoI/AA4Btid5kp2n2mvB7r6pqrYleSdwFTALuLSqbkpyXvP6xcB/BI4C/rLJmW1VtWqvWzMEewSSNNEwdxYftrdvXlVX0lvIpn/fxX3Pz6E3hcV+MTbGNF1kU5LaM8wNZUny5iT/odk+vpl4btoJsUcgSZMMM0bwl/RuIvvVZvsJejeKTTtj3lAmSVMMM0bw4qo6Ocl3AKrqkWbwd9pxsFiSphqmR7C1mS6iAJIsBna0WlVbvHxUkqYYJgguBD4HHJ3kPwNfB/5Lq1W1ZCxxsFiSJhnmqqFPJrkeeBW9S0dfV1U3t15ZC1yhTJKmetogSPJB4H9X1bQcIO7nFBOSNNUwp4a+DbyvWVPg/UlaveGrTU4xIUlTPW0QVNXfVNWZ9KaVvhW4IMltrVfWgnjVkCRNMUyPYNyJwPOAZcAtrVTTMqehlqSphrmzeLwH8CfATcDPVtUvtV5ZC8bGcIxAkiYZ5oayO4FTq+qhtotpm1NMSNJUw1w+enGSI5r5heb17f9aq5W1YMzbCCRpimEuHz0HeBe9hWVuAF4CXAO8st3S9r0kzjUkSZMMM1j8LuBFwN1V9XPAC4G9Xzh4hBIHiyVpsmGC4KmqegogyUFVdQvw3HbLaoc3lEnSVMMMFq9PshD4PPClJI8wYO3h6cApJiRpqmEGi1/fPP1PSb4CHA58sdWqWmKPQJKmGqZHsFNVfbWtQvYLp5iQpCn25M7iac/LRyVpqk4FQYhXDUnSJJ0KAqeYkKSpOhUETjEhSVN1KghmjYXt3losSRN0Kgjmzh5j87Ydoy5Dkg4onQuCLdsNAknq16kgOGjWGFvsEUjSBJ0KgrmzDQJJmqxzQeAYgSRN1K0g8NSQJE3RrSBwsFiSpuhcEGzfUd5LIEl9OhcEgKeHJKlPt4JglkEgSZN1KggWzJsDwGNPbh1xJZJ04Gg1CJKcnmRtknVJzh/w+vOSXJNkc5J/12YtAMctnA/AfY892faPkqRpY49WKNsTSWYBFwGvAdYD1yW5oqp+0HfYj4HfA17XVh39jls4D4B7HzEIJGlcmz2CU4B1VXVHVW0BLgdW9x9QVQ9W1XXAfjlXs7NH8KhBIEnj2gyCJcA9fdvrm317LMm5SdYkWbNhw4a9LmjenFksOnSup4YkqU+bQZAB+/bqAv6quqSqVlXVqsWLFz+jopYsnM+9jz71jN5DkmaSNoNgPXB83/ZS4L4Wf95Qjls4n3sf2TTqMiTpgNFmEFwHrEiyPMlc4GzgihZ/3lCOWzif+x59ykXsJanR2lVDVbUtyTuBq4BZwKVVdVOS85rXL05yDLAGWADsSPL7wMqqerytuk448mCe3LqdDU9s5ujD5rX1YyRp2mgtCACq6krgykn7Lu57/gC9U0b7zbJFhwBw10ObDAJJomN3FgMsO+pgAO566CcjrkSSDgydC4IlC+czeyzc9bBBIEnQwSCYPWuM44882CCQpEbnggB6p4fufMhLSCUJuhoEiw7h7od/4iWkkkRXg+CoQ9i0ZTsbNm4edSmSNHKdDILlzSWkt29wnECSOhkEK551KAC3PbhxxJVI0uh1MgiOWTCPw+bN5tYfGQSS1MkgSMK/fNZh3PrAE6MuRZJGrpNBAPSC4MGNXjkkqfM6HASH8uimrV45JKnzOhsEz33WYQDc+iNPD0nqts4GwYomCNY6YCyp4zobBIsOncsRB8/hNoNAUsd1NgiS8NxjDuPm+1tbA0eSpoXOBgHASUsXcvP9G9mybceoS5Gkkel4EBzOlu07uOUBewWSuqvTQfD8pQsB+O76x0ZciSSNTqeDYOkR8zni4DnceM+joy5Fkkam00GQhJOWLuRGewSSOqzTQQDw/KWHc9uDG9m0ZduoS5Gkkeh8EJy0dCE7CnsFkjqr80HwomVHksA1tz886lIkaSQ6HwSHHzyHk5Yczj+te2jUpUjSSHQ+CABOO3ER37nnUTY+tXXUpUjSfmcQAC9bsYjtO4pr7/jxqEuRpP3OIABOPuEI5s0Z4+ueHpLUQQYBMG/OLE5ZfhRfWfugK5ZJ6hyDoPHanzmWux/e5GWkkjrHIGj8wk8fw9xZY3z+hntHXYok7VcGQePw+XN49cqj+dx37uWprdtHXY4k7TcGQZ+3nLqMRzdt5Yob7ht1KZK03xgEfV68/EhWHruAD31lnYvVSOoMg6BPEt59xvP44Y838ZGv3zHqciRpvzAIJnnFikWc8dPH8OdX38q1dzj/kKSZr9UgSHJ6krVJ1iU5f8DrSXJh8/qNSU5us55hJOGCN57ECUcezNs+dh1X3fTAqEuSpFa1FgRJZgEXAWcAK4E3JVk56bAzgBXN17nAh9uqZ08smDeHy97xEp6z+FB+6xPX86ZLvsll3/ohN9//OE9s3uZNZ5JmlNktvvcpwLqqugMgyeXAauAHfcesBj5evU/WbyZZmOTYqrq/xbqGcszh8/i7807lE9fczSe+eTfv+ez3dr42Z1ZYMG8Os8bC7LEw1veYlupJ2nnntuqVtO+d9aLjOeflz9nn79tmECwB7unbXg+8eIhjlgATgiDJufR6DJxwwgn7vNBdmTdnFu94xXM45+XLuX3DE/zg/o088NiTPLJpKxuf2sr2HcW27cX2qt7zHS31FFp7W3s20nSy6NCDWnnfNoNg0B+bkz95hjmGqroEuARg1apV+/3TKwknHn0YJx592P7+0ZLUujYHi9cDx/dtLwUm36k1zDGSpBa1GQTXASuSLE8yFzgbuGLSMVcAb2muHnoJ8NiBMD4gSV3S2qmhqtqW5J3AVcAs4NKquinJec3rFwNXAmcC64BNwFvbqkeSNFibYwRU1ZX0Puz7913c97yA32mzBknS7nlnsSR1nEEgSR1nEEhSxxkEktRxmW7z5iTZANy9l9++CHhoH5YzHdjmbrDN3fBM2vzsqlo86IVpFwTPRJI1VbVq1HXsT7a5G2xzN7TVZk8NSVLHGQSS1HFdC4JLRl3ACNjmbrDN3dBKmzs1RiBJmqprPQJJ0iQGgSR1XGeCIMnpSdYmWZfk/FHXs68kOT7JV5LcnOSmJO9q9h+Z5EtJbmsej+j7nvc0v4e1SX5hdNXvvSSzknwnyRea7Zne3oVJPp3klubf+tQOtPnfNv9Nfz/JZUnmzbQ2J7k0yYNJvt+3b4/bmORnk3yvee3C7OnatlU147/oTYN9O/AcYC7wXWDlqOvaR207Fji5eX4YcCuwEvjvwPnN/vOBC5rnK5v2HwQsb34vs0bdjr1o9x8Afwt8odme6e39G+Cc5vlcYOFMbjO9JWvvBOY3258CfnOmtRl4BXAy8P2+fXvcRuBbwKn0Vn38B+CMPamjKz2CU4B1VXVHVW0BLgdWj7imfaKq7q+qbzfPNwI30/ufaDW9Dw+ax9c1z1cDl1fV5qq6k95aEKfs36qfmSRLgV8EPtK3eya3dwG9D4yPAlTVlqp6lBnc5sZsYH6S2cDB9FYvnFFtrqqvAT+etHuP2pjkWGBBVV1TvVT4eN/3DKUrQbAEuKdve32zb0ZJsgx4IXAt8KxqVntrHo9uDpsJv4sPAH8E7OjbN5Pb+xxgA/DXzemwjyQ5hBnc5qq6F/gfwA+B++mtXng1M7jNffa0jUua55P3D60rQTDofNmMum42yaHAZ4Dfr6rHd3fogH3T5neR5LXAg1V1/bDfMmDftGlvYza90wcfrqoXAj+hd8pgV6Z9m5vz4qvpnQI5DjgkyZt39y0D9k2rNg9hV218xm3vShCsB47v215Kr5s5IySZQy8EPllVn212/6jpMtI8Ptjsn+6/i9OAf5PkLnqn+F6Z5H8xc9sLvTasr6prm+1P0wuGmdzmVwN3VtWGqtoKfBZ4KTO7zeP2tI3rm+eT9w+tK0FwHbAiyfIkc4GzgStGXNM+0Vwd8FHg5qr6i76XrgB+o3n+G8D/6dt/dpKDkiwHVtAbaJoWquo9VbW0qpbR+3f8x6p6MzO0vQBV9QBwT5LnNrteBfyAGdxmeqeEXpLk4Oa/8VfRG/+ayW0et0dtbE4fbUzykuZ39Za+7xnOqEfN9+Po/Jn0rqi5HXjvqOvZh+16Gb1u4I3ADc3XmcBRwJeB25rHI/u+573N72Ete3h1wYH0Bfxr/vmqoRndXuAFwJrm3/nzwBEdaPMfA7cA3wc+Qe9qmRnVZuAyemMgW+n9Zf/2vWkjsKr5Pd0OfIhm1ohhv5xiQpI6riunhiRJu2AQSFLHGQSS1HEGgSR1nEEgSR1nEEiTJNme5Ia+r302W22SZf0zTUoHgtmjLkA6AD1ZVS8YdRHS/mKPQBpSkruSXJDkW83Xic3+Zyf5cpIbm8cTmv3PSvK5JN9tvl7avNWsJH/VzLV/dZL5I2uUhEEgDTJ/0qmhs/pee7yqTqF39+YHmn0fAj5eVScBnwQubPZfCHy1qp5Pb26gm5r9K4CLquqngEeBN7TcHmm3vLNYmiTJE1V16ID9dwGvrKo7mon+Hqiqo5I8BBxbVVub/fdX1aIkG4ClVbW57z2WAV+qqhXN9ruBOVX1Z+23TBrMHoG0Z2oXz3d1zCCb+55vx7E6jZhBIO2Zs/oer2mef4PeTKgAvwZ8vXn+ZeC3Yecaywv2V5HSnvAvEWmq+Ulu6Nv+YlWNX0J6UJJr6f0R9aZm3+8Blyb5Q3orib212f8u4JIkb6f3l/9v05tpUjqgOEYgDakZI1hVVQ+NuhZpX/LUkCR1nD0CSeo4ewSS1HEGgSR1nEEgSR1nEEhSxxkEktRx/x/KaaylyVKmgAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# visualization\n",
    "# reduce Dimensionality to a single value per epoch\n",
    "\n",
    "average_loss= np.mean(loss,1)\n",
    "\n",
    "# plot the loss over all epochs\n",
    "plt.plot(average_loss)\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"average loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
